sample <- sample(data, 100000)
library(stringr)
sample <- sapply(sample, str_replace_all, pattern = "[^a-zA-z ]", replacement = "")
sample <- sapply(sample, tolower)
con <- file("en_US/en_US.blogs.txt", "r")
blogs <- readLines(con)
close(con)
con <- file("en_US/en_US.news.txt", "r")
news <- readLines(con)
close(con)
con <- file("en_US/en_US.twitter.txt", "r")
twitter <- readLines(con)
close(con)
data <- c(blogs, news, twitter)
set.seed(0)
sample <- sample(data, 100000)
library(stringr)
sample <- sapply(sample, str_replace_all, pattern = "[^a-zA-z ]", replacement = "")
sample <- sapply(sample, tolower)
con <- file("sample.txt", "w")
writeLines(sample, con)
close(con)
head(data)
tail(data)
str_replace_all(data[1], pattern = "[^a-zA-z ]", replacement = "")
data <- sapply(data, str_replace_all, pattern = "[^a-zA-z ]", replacement = "")
con <- file("sample.txt", "r")
data <- readLines(con)
close(con)
head(data)
tail(data)
suppressMessages(library(tm))
vs <- sapply(data, VectorSource)
corpus <- VCorpus(vs)
vs <- VectorSource(data)
corpus <- VCorpus(vs)
corpus[[1]]$content
gram1 <- TermDocumentMatrix(corpus)
gram1.freq
gram1.freq <- freq_df(corpus.unigram)
?gram1
?TermDocumentMatrix
con <- file("sample.txt", "r")
data <- readLines(con)
close(con)
head(data)
tail(data)
suppressMessages(library(tm))
vs <- sapply(data, VectorSource)
corpus <- VCorpus(vs)
con <- file("sample.txt", "r")
data <- readLines(con)
close(con)
suppressMessages(library(tm))
vs <- sapply(data, VectorSource)
corpus <- VCorpus(vs)
library(RWeka)
suppressMessages(library(RWeka))
suppressMessages(library(tm))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(tdm)
suppressMessages(library(RWeka))
suppressMessages(library(tm))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(data, control = list(tokenize = BigramTokenizer))
inspect(tdm)
?TermDocumentMatrix
corpus = Corpus(data)
corpus = Corpus(singleTextDocSource("sample.txt"))
?Corpus
a  <-VCorpus(DirSource("sample.txt"))
a  <-VCorpus(DirSource(".\sample.txt"))
a  <-VCorpus(DirSource(".\\sample.txt"))
getwd()
a  <-VCorpus(DirSource("D:/黄震的电脑/大学/留学/final/sample.txt"))
con <- file("sample.txt", "r")
a  <-VCorpus(DirSource(con))
a  <-VCorpus(Source(con))
a  <-VCorpus(SimpleSource(con))
a  <-VCorpus(SimpleSource(con), encoding = "utf-8")
a  <-VCorpus(VectorSource(data))
suppressMessages(library(RWeka))
suppressMessages(library(tm))
corpus <- VCorpus(VectorSource(data))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
inspect(tdm)
inspect(tdm[300:400,1:100])
?removeSparseTerms
gramFreqs <- function(tdm){
tdm <- removeSparseTerms(tdm, 0.999)
freq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
freq <- data.frame(word=names(freq), freq=freq)
return(freq)
}
gramFreqs(tdm)
corpus
?courpus
?Corpus
gramFreqs <- function(tdm){
tdm <- removeSparseTerms(tdm, 0.9)
freq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
freq <- data.frame(word=names(freq), freq=freq)
return(freq)
}
head(gramFreqs(tdm1), n = 10)
head(gramFreqs(tdm1), n = 10)
head(gramFreqs(tdm1), n = 10)
head(gramFreqs(tdm), n = 10)
?hist
?hist
help(hist)
?hist
corpus <- VCorpus(VectorSource(data))
library(RWeka)
library(tm)
suppressMessages(library(RWeka))
suppressMessages(library(tm))
con <- file("sample.txt", "r")
data <- readLines(con)
close(con)
corpus <- VCorpus(VectorSource(data))
tdm1 <- TermDocumentMatrix(corpus)
tdm1 <- removeSparseTerms(tdm1, 0.9)
gramFreqs <- function(tdm){
freq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
freq <- data.frame(word=names(freq), freq=freq)
return(freq)
}
freqs1 <- gramFreqs(tdm1)
freqs1
freqs1.freq
freqs1$freq
hist(freqs1$freq)
barplot(freqs1$freq)
barplot(freqs1$freq, labels = freqs1$word)
?barplot
con <- file("en_US/en_US.blogs.txt", "r")
blogs <- readLines(con)
close(con)
con <- file("en_US/en_US.news.txt", "r")
news <- readLines(con)
close(con)
con <- file("en_US/en_US.twitter.txt", "r")
twitter <- readLines(con)
close(con)
sum(sapply(gregexpr("\\S+", x), length))
sum(sapply(gregexpr("\\S+", blogs), length))
message("hi", "im")
con <- file("en_US/en_US.blogs.txt", "r")
blogs <- readLines(con)
close(con)
con <- file("en_US/en_US.news.txt", "r")
news <- readLines(con)
close(con)
con <- file("en_US/en_US.twitter.txt", "r")
twitter <- readLines(con)
close(con)
lines <- c(899288, 77259, 2360148)
words <- c(37570839, 34494539, 30451128)
size <- c("231.3MB", "17.7MB", "257.2MB")
res <- data.frame(lines, words, size)
res
lines <- c(899288, 77259, 2360148)
words <- c(37570839, 34494539, 30451128)
size <- c("231.3MB", "17.7MB", "257.2MB")
res <- data.frame(lines, words, size)
row.names(res) <- c("blogs", "news", "twitter")
res
con <- file("en_US/en_US.blogs.txt", "r")
blogs <- readLines(con)
close(con)
con <- file("en_US/en_US.news.txt", "r")
news <- readLines(con)
close(con)
con <- file("en_US/en_US.twitter.txt", "r")
twitter <- readLines(con)
close(con)
getwd()
con <- file("en_US/en_US.blogs.txt", "r")
blogs <- readLines(con)
close(con)
con <- file("en_US/en_US.news.txt", "r")
news <- readLines(con)
close(con)
con <- file("en_US/en_US.twitter.txt", "r")
twitter <- readLines(con)
close(con)
data <- c(blogs, news, twitter)
set.seed(0)
sample <- sample(data, 100000)
set.seed(0)
sample <- sample(blogs, 100000)
library(stringr)
sample <- sapply(sample, str_replace_all, pattern = "[^a-zA-z ]", replacement = "")
sample <- sapply(sample, tolower)
con <- file("blogssample.txt", "w")
writeLines(sample, con)
close(con)
set.seed(0)
sample <- sample(twitter, 100000)
library(stringr)
sample <- sapply(sample, str_replace_all, pattern = "[^a-zA-z ]", replacement = "")
sample <- sapply(sample, tolower)
con <- file("twittersample.txt", "w")
writeLines(sample, con)
close(con)
set.seed(0)
sample <- sample(news, 100000)
sample <- news
library(stringr)
sample <- sapply(sample, str_replace_all, pattern = "[^a-zA-z ]", replacement = "")
sample <- sapply(sample, tolower)
con <- file("newssample.txt", "w")
writeLines(sample, con)
close(con)
shiny::runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
help(h4)
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
help(column)
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
help("updateTextAreaInput")
runApp('text_mining')
runApp('text_mining')
runApp('D:/text_prediction')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
help(column)
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
shiny::runApp('text_mining')
con <- file("blogssample.txt", "r")
blogs <- readLines(con)
close(con)
con <- file("twittersample.txt", "r")
twitter <- readLines(con)
close(con)
blogstdm <- TermDocumentMatrix(sample)
blogstdm <- removeSparseTerms(blogstdm, 0.99)
blogstdm <- as.matrix(blogstdm)
library(shiny)
library(RWeka)
library(tm)
library(wordcloud)
blogstdm <- TermDocumentMatrix(sample)
blogstdm <- removeSparseTerms(blogstdm, 0.99)
blogstdm <- as.matrix(blogstdm)
blogstdm <- TermDocumentMatrix(blogs)
blogstdm <- removeSparseTerms(blogstdm, 0.99)
blogstdm <- as.matrix(blogstdm)
blogstdm <- TermDocumentMatrix(VectorSource(blogs))
blogstdm <- removeSparseTerms(blogstdm, 0.99)
blogstdm <- as.matrix(blogstdm)
write.csv(df, "sample.csv")
source('D:/final/csvgenerator.R')
source('D:/final/csvgenerator.R')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
wordcloud?
)
help("wordcloud")
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
help("barplot")
runApp('text_mining')
help("head")
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
install.packages("ggplot")
install.packages("ggplot2")
install.packages("ggplot2")
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
help("geom_bar")
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
library(stringi)
help("observe")
help("observeEvent")
help(observe)
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
help("sidebarLayout")
runApp('text_mining')
help("sliderInput")
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
help("radioButtons")
help("submitButton")
runApp('text_mining')
runApp('text_mining')
source('D:/final/csvgenerator.R')
source('D:/final/csvgenerator.R')
source("global.R")
getwd()
a = c("a", "b")
a[0]
a[1]
help("eventReactive")
set.seed(0)
randu
sample(1, max(3, 4), 1)
sample(1:max(3, 4), 1)
sample(1:max(3, 4), 1)
sample(1:max(3, 4), 1)
sample(1:max(3, 4), 1)
sample(1:max(3, 4), 1)
sample(1:max(3, 4), 1)
sample(1:max(3, 4), 1)
len(a)
length(a)
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
source('D:/final/datagenerator.R')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
predict <- c("a")
predict <- c(predict, "b")
predict
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
c("little", c("was", "a"))
predict <- c("little", c("was", "a"))
predict
c(c("a","b"), c("c", "d"))
shiny::runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
shiny::runApp('text_mining')
sample1 <- read.csv("sample1.csv")
sample2 <- read.csv("sample2.csv")
sample3 <- read.csv("sample3.csv")
data1 <- sample1
data2 <- sample2
data3 <- sample3
words = c("i", "was", "a")
predict <- predict3(words[[length(words) - 2]], words[[length(words) - 1]],
words[[length(words)]], data3)
predict
if(length(predict) < 3) {
print(words[[length(words) - 1]])
print(words[[length(words)]])
predict <- c(predict, predict2(words[[length(words) - 1]],
words[[length(words)]], data2))
}
predict
predict2(words[[length(words) - 1]]
words[[length(words)]], data2)
predict2(words[[length(words) - 1]],
words[[length(words)]], data2))
predict2(words[[length(words) - 1]],
words[[length(words)]], data2)
predict2("was", "a", data())
predict2("was","a", data2)
predict
if(length(predict) < 3) {
print(words[[length(words)]])
predit <- predict1(words[[length(words)]], data1)
predit <- c(predict, predict1(words[[length(words)]], data1))
}
predict
predict1(words[[length(words)]], data1)
c("little", c("few", "lot", "good", "little"))
p1 <- predict1(words[[length(words)]], data1)
p1
predict = c(predict, p1)
predict
unique(predict)
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
sentence = "I am a pig     "
simplified <- gsub("\\s+"," ",sentence)
simplified
runApp('text_mining')
grepl("[:alpha:]", "a")
grepl("[:alpha:]", "A")
grepl("[a-zA-Z]", "a")
grepl("[a-zA-Z]", "A")
grepl("[a-zA-Z]", ".")
help(sapply)
runApp('text_mining')
sentence = ""
simplified <- gsub("\\s+"," ",sentence)sentence
simplified <- gsub("\\s+"," ",sentence)
simplified == ""
sentence = " "
simplified <- gsub("\\s+"," ",sentence)
simplified
runApp('text_mining')
sentence = "I am a pig"
simplified <- gsub("\\s+"," ",sentence)
last = length(simplified)
simplified
length(simplified)
simplified <- gsub("\\s+"," ",sentence)[[1]]
length(simplified)
simplified
nchar(simplified)
simplified <- gsub("\\s+"," ",input$sentence)
simplified <- gsub("\\s+"," ",sentence)
nchar(simplified)
runApp('text_mining')
runApp('text_mining')
simplified <- gsub("\\s+"," ",sentence)
last = nchar(simplified)
simplified[last]
simplified
substr(simplified, last, last)
runApp('text_mining')
runApp('text_mining')
last = nchar(simplified)
substr(simplified, last, last) == " "
substr(simplified, last, last)
grepl("[a-zA-Z]", substr(simplified, last, last))
length(simplified)
runApp('text_mining')
runApp('text_mining')
sentence = "I "
simplified <- gsub("\\s+"," ",sentence)
length(simplified) == 0
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
runApp('text_mining')
xlab("Word") + ylab("Frequency")
install.packages("RCurl")
install.packages("RJSONIO")
install.packages("PKI")
install.packages("rstudioapi")
install.packages("packrat")
install.packages("rsconnect")
runApp('text_mining')
